{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to try next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, I have done SVM testing on my data, as well as K-Means, PCA, and GNB.\n",
    "\n",
    "In the Python handbook the others methods it suggests are:\n",
    "- Linear Regression\n",
    "- Decision Trees\n",
    "- Manifold Learning\n",
    "- Gaussian Mixture Models\n",
    "- Kernel Density Estimation\n",
    "\n",
    "So let's have a brief look at what each of the above do, exactly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "\n",
    "> \"Just as naive Bayes (discussed earlier in In Depth: Naive Bayes Classification) is a good starting point for classification tasks, linear regression models are a good starting point for regression tasks. Such models are popular because they can be fit very quickly, and are very interpretable. You are probably familiar with the simplest form of a linear regression model (i.e., fitting a straight line to data) but such models can be extended to model more complicated data behavior.\"\n",
    "\n",
    "Looking back at workbook \"20190604 - Classifications or Clusters?\" I can see that my research is more suited to _supervised learning_ (which encompasses classification tasks (labels are discrete categories) and regression tasks (labels are continuous categories)) and thus regression-based tasks would be suitable for my research.\n",
    "\n",
    "However, this is just using the raw data given to me - we have already talked about eventually using the Light Curves instead of data gathered in a database, as the Light Curves are the _measured_ or _observed_ data, whereas (we hypothesise) most of the data obtained from the datasets I have been using is _calculated_ or _computed_ data.\n",
    "\n",
    "But does that necessarily matter _right now_ where I am still essentially learning how to work with the Machine Learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees (and random forests)\n",
    "\n",
    ">\"Previously we have looked in depth at a simple generative classifier (naive Bayes; see In Depth: Naive Bayes Classification) and a powerful discriminative classifier (support vector machines; see In-Depth: Support Vector Machines). Here we'll take a look at motivating another powerful algorithm—a non-parametric algorithm called random forests. Random forests are an example of an ensemble method, meaning that it relies on aggregating the results of an ensemble of simpler estimators. The somewhat surprising result with such ensemble methods is that the sum can be greater than the parts: that is, a majority vote among a number of estimators can end up being better than any of the individual estimators doing the voting!\"\n",
    "\n",
    "Decision trees seem to work best where each \"decision\" has precisely two options. Having multiple options works in theory, and in practice, and in graphs, etc (ex: flowcharts) but from a brief look it seems that the algorithm here works best when splitting a group in two; or, each \"decision\" has two \"results\" to perform a binary split.\n",
    "\n",
    "Whether this will be as useful for my data (where I am essentially looking at \"Hot/Cold/Right\") is unknown, as I have not given a huge amount of thought to this as of yet, but it is possible that it would come in very handy for decisions I am not yet thinking of.\n",
    "\n",
    "On a (once again, very brief) overview, it seems this is less useful than the others, but I won't immediately discount it as it has not been tried."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manifold Learning\n",
    "\n",
    ">\"We have seen how principal component analysis (PCA) can be used in the dimensionality reduction task—reducing the number of features of a dataset while maintaining the essential relationships between the points. While PCA is flexible, fast, and easily interpretable, it does not perform so well when there are nonlinear relationships within the data; we will see some examples of these below.\n",
    "To address this deficiency, we can turn to a class of methods known as manifold learning—a class of unsupervised estimators that seeks to describe datasets as low-dimensional manifolds embedded in high-dimensional spaces. When you think of a manifold, I'd suggest imagining a sheet of paper: this is a two-dimensional object that lives in our familiar three-dimensional world, and can be bent or rolled in that two dimensions.\"\n",
    "\n",
    "Manifold learning seems to be about dimensional reduction, or rather, finding a lower-dimension way of describing higher dimensional data points. This has already proven useful in PCA observations, but for my data PCA had some interesting results - see workbook \"20190622 - Test on known data\" close to the bottom for examples. Perhaps this is because I am working with data that cannot be _linearly_ reduced to 2 dimensions - something that Manifold Learning seems to be designed for.\n",
    "\n",
    "Given the types of data, and the eventual want to move to Light Curves as opposed to Dataset data, this is something that could prove to be very useful, however - once again - I have only given a brief look at what is happening, and have not tried anything just yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Mixture Models\n",
    "\n",
    ">\"The k-means clustering model explored in the previous section is simple and relatively easy to understand, but its simplicity leads to practical challenges in its application. In particular, the non-probabilistic nature of k-means and its use of simple distance-from-cluster-center to assign cluster membership leads to poor performance for many real-world situations. In this section we will take a look at Gaussian mixture models (GMMs), which can be viewed as an extension of the ideas behind k-means, but can also be a powerful tool for estimation beyond simple clustering.\"\n",
    "\n",
    "K-means culstering seems to be one of the best ways to identify clusters in simple datasets (ex: Iris dataset) with 2-dimensional data (often used in combination with PCA measures to reduce dimensionality). GMM seems to be the \"bigger brother\" to k-means.\n",
    "\n",
    "Additionally, further investigation shows that whilst k-means uses a system whereby it calculates a cluster centre, and the radius of said cluster is equal to the point farthest away from the centre that is still in the cluster. This gives us circular (or n-spherical, in higher dims) clusters - _always_. The clusters are _always_ circular, without exception. This might not be ideal, and GMM addresses this by being able to define clusters as ovals (or n-ovoids(?) in higher dims) by giving 'weights' to each point, where weight is synonymous for percentage certainty that the point belongs to the cluster. It then defines the cluster based on gaussian distributions of the cluster using all the points in it (not just the centre of the cluster) and taking into account these weights.\n",
    "\n",
    "Looking back at workbook \"20190614-1 - Introducing Scikit-Learn\" close to the bottom, a GMM model is used to calculate the clusters of the iris dataset, and then a plot is made of each cluster individually. I am sure this is also done with other models, but this is how I've seen it, and so - for now - I'm gonna take the coconuts out and try to summon that plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Density Estimation\n",
    "\n",
    ">\"In the previous section we covered Gaussian mixture models (GMM), which are a kind of hybrid between a clustering estimator and a density estimator. Recall that a density estimator is an algorithm which takes a $D$-dimensional dataset and produces an estimate of the $D$-dimensional probability distribution which that data is drawn from. The GMM algorithm accomplishes this by representing the density as a weighted sum of Gaussian distributions. Kernel density estimation (KDE) is in some senses an algorithm which takes the mixture-of-Gaussians idea to its logical extreme: it uses a mixture consisting of one Gaussian component per point, resulting in an essentially non-parametric estimator of density. In this section, we will explore the motivation and uses of KDE.\"\n",
    "\n",
    "I'm not going to pretend I understand this immediately, and I have looked at this one the least, but considering the openi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
