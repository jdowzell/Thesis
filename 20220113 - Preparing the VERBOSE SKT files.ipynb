{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f9da32-0251-4c97-bb84-429ba7503938",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# General Imports\n",
    "################################\n",
    "import csv, math, io, os, os.path, sys, random, time, json, gc, glob\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "\n",
    "################################\n",
    "# Scientific Imports\n",
    "################################\n",
    "import scipy\n",
    "from scipy.signal import butter,filtfilt\n",
    "\n",
    "################################\n",
    "# SKLearn Imports\n",
    "################################\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "################################\n",
    "# SKTime Imports\n",
    "################################\n",
    "from sktime.datatypes._panel._convert import from_2d_array_to_nested, from_nested_to_2d_array, is_nested_dataframe\n",
    "from sktime.forecasting.compose import TransformedTargetForecaster\n",
    "from sktime.forecasting.model_selection import ForecastingGridSearchCV\n",
    "\n",
    "from sktime.classification.kernel_based import Arsenal\n",
    "from sktime.classification.interval_based import CanonicalIntervalForest\n",
    "from sktime.classification.dictionary_based import ContractableBOSS\n",
    "from sktime.classification.interval_based import DrCIF\n",
    "from sktime.classification.hybrid import HIVECOTEV1\n",
    "from sktime.classification.dictionary_based import IndividualBOSS\n",
    "from sktime.classification.dictionary_based import IndividualTDE\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from sktime.classification.feature_based import MatrixProfileClassifier\n",
    "from sktime.classification.dictionary_based import MUSE\n",
    "from sktime.classification.interval_based import RandomIntervalSpectralForest\n",
    "from sktime.classification.distance_based import ShapeDTW\n",
    "from sktime.classification.feature_based import SignatureClassifier\n",
    "from sktime.classification.interval_based import SupervisedTimeSeriesForest\n",
    "from sktime.classification.feature_based import TSFreshClassifier\n",
    "from sktime.classification.dictionary_based import WEASEL\n",
    "\n",
    "################################\n",
    "# Suppress Warnings\n",
    "################################\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n",
    "\n",
    "################################\n",
    "# Initialisers\n",
    "################################\n",
    "default_rc_params = (16,9)\n",
    "plt.rcParams[\"figure.figsize\"] = default_rc_params\n",
    "sb.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49c30b97-b634-4a9d-8093-68bfb653f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Data Initialisers\n",
    "################################\n",
    "xNaNs = np.load(\"X_NAN_LIST.npy\")\n",
    "xTime = np.load(\"X_TIME_LIST.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8c2d67e-3f9f-4104-9062-b37a713674fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#masterX = [x[1:-1] for x in np.load(\"None_Or_One_Exoplanet_NORMALISED.npy\")]\n",
    "masterX = [x[1:-1] for x in np.load(\"None_Or_One_Exoplanet.npy\")]\n",
    "masterY = np.load(\"None_Or_One_isplanetlist.npy\")\n",
    "\n",
    "#X_nested = from_2d_array_to_nested(np.array(masterX))\n",
    "#Xtrain, Xtest, ytrain, ytest  = train_test_split(X_nested, masterY, random_state=42)\n",
    "Xtrain_,Xtest_,ytrain_,ytest_ = train_test_split (masterX, masterY, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca434397-0559-414a-9b5e-7203c8931744",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Functions\n",
    "################################\n",
    "\n",
    "def GetLCData(rndFile=-1, outputFileName=False):\n",
    "    \n",
    "    # rndFile is random, unless specified\n",
    "    rndFile = random.randint(0,len(fitsarr)) if rndFile==-1 else rndFile\n",
    "    \n",
    "    # Get LC data from the requisite fits file\n",
    "    fitsFile = fitsarr[rndFile]\n",
    "\n",
    "    # The following line of code gives us the header values\n",
    "    fitsHeaders = fits.getheader(fitsFile)\n",
    "\n",
    "    with fits.open(fitsFile, mode=\"readonly\") as hdulist:\n",
    "\n",
    "        tess_bjds     = hdulist[1].data['TIME']\n",
    "        pdcsap_fluxes = hdulist[1].data['PDCSAP_FLUX']\n",
    "    \n",
    "    if outputFileName:\n",
    "        return (tess_bjds[1:-1], pdcsap_fluxes[1:-1], rndFile)\n",
    "    else:\n",
    "        return (tess_bjds[1:-1], pdcsap_fluxes[1:-1])\n",
    "\n",
    "################################\n",
    "\n",
    "def Every_Nth_Value(y,n=40):\n",
    "    return (y[::n])\n",
    "\n",
    "################################\n",
    "\n",
    "def Every_Nth_ValueXY(x,y,n=40):\n",
    "    return (Every_Nth_Value(x,n), Every_Nth_Value(y,n))\n",
    "\n",
    "################################\n",
    "\n",
    "def GetNumDays(time=xTime):\n",
    "    \n",
    "    #xTime = np.load(\"X_TIME_LIST.npy\")\n",
    "    nDays = time[-1]-time[0]\n",
    "    \n",
    "    return (nDays)\n",
    "\n",
    "################################\n",
    "\n",
    "def FilterMyData(x,cutoff=0.00005,order=2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to apply a Butter Filter to timeseries.\n",
    "    Vars:\n",
    "    \n",
    "    y:        The timeseries. Must be list or np array.\n",
    "    cutoff:   The cutoff frequency. Used to determine where the filter cut off is.\n",
    "    order:    Approximation via polynomial of the order'th degree (2=quadratic, 3=cubic, 4=quartic, etc)\n",
    "    \"\"\"\n",
    "    \n",
    "    # DATA VALIDATION\n",
    "    \n",
    "    # Flag\n",
    "    isNested = False\n",
    "    \n",
    "#####    # Check to see if x is a nested dataframe or not\n",
    "#####    if type(x) == pd.core.frame.DataFrame:\n",
    "#####        isNested = True\n",
    "#####        print(\"NESTED DATAFRAME FOUND! UNPACKING FOR CALCULATIONS, THE REPACKING...\")\n",
    "#####        x = from_nested_to_2d_array(x)\n",
    "    \n",
    "    # First, let's calculate the observational time period;\n",
    "    # This is done separately so that I can change this in the future for any TESS fits file\n",
    "    numdays       = GetNumDays()\n",
    "    \n",
    "    # Next, fix data\n",
    "    xMedian       = np.median(x)                                                    # Get the median value of 'x' before changing it\n",
    "    x             = [xMedian if n in xNaNs else item for n,item in enumerate(x)]    # Change all the missing values to the median value of the whole array\n",
    "    \n",
    "    # Frequency Data Stuff\n",
    "    sec           = numdays*24*60*60   # Number of seconds in the overall observation period\n",
    "    freq          = len(x)/sec         # Frequency, in Hz, ie number of observations per second\n",
    "    # FREQ IS APPROX 1/120 OR ~0.008333333\n",
    "    \n",
    "    # Butter Lowpass Filter\n",
    "    polynomOrder  = order\n",
    "    nyq           = 0.5 * freq\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a          = butter(polynomOrder, normal_cutoff, btype='low', analog=False)\n",
    "    newX          = filtfilt(b, a, x)\n",
    "    \n",
    "    if isNested == True:\n",
    "        nexX = from_2d_array_to_nested(newX)\n",
    "    \n",
    "    # Finally, return the new X and Y values\n",
    "    return (newX)\n",
    "\n",
    "################################\n",
    "\n",
    "def GetFreqData(x,cutoff=0.00005):\n",
    "    \n",
    "    # First, let's calculate the observational time period;\n",
    "    # This is done separately so that I can change this in the future for any TESS fits file\n",
    "    numdays       = GetNumDays()\n",
    "    \n",
    "    # Next, fix data                           \n",
    "    xMedian       = np.median(x)                                                    # Get the median value of 'y' before changing it\n",
    "    x             = [xMedian if n in xNaNs else item for n,item in enumerate(x)]    # Change all the missing values to the median value of the whole array\n",
    "    \n",
    "    # Frequency Data Stuff\n",
    "    sec           = numdays*24*60*60   # Number of seconds in the overall observation period\n",
    "    freq          = len(x)/sec         # Frequency, in Hz, ie number of observations per second\n",
    "    # FREQ IS APPROX 1/120 OR ~0.008333333\n",
    "    \n",
    "    # Butter Lowpass Filter\n",
    "    nyq           = 0.5 * freq\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    \n",
    "    # Finally, return the new X and Y values\n",
    "    return (freq,normal_cutoff)\n",
    "\n",
    "################################\n",
    "\n",
    "def FIXNAN(y, nanList=xNaNs):\n",
    "    yMedian = np.median(y)\n",
    "    y = [yMedian if n in nanList else item for n,item in enumerate(y)]\n",
    "    return y\n",
    "\n",
    "################################\n",
    "\n",
    "def GetMetrics(classifierType, Xtrain, Xtest, ytrain, ytest, param_grid):\n",
    "    \n",
    "    # Make a PCA Pipeline\n",
    "    print(\"> GM: START\")\n",
    "    \n",
    "    model = classifierType()\n",
    "    cname = classifierType.__name__\n",
    "    print(f\"\\t> Model: {cname}\")\n",
    "    \n",
    "    print(\"> GM: GENERATING TRANSFORMERS\")\n",
    "    flt = FunctionTransformer(FilterMyData)\n",
    "    nth = FunctionTransformer(Every_Nth_Value)\n",
    "    \n",
    "    print(\"> GM: MAKE PIPELINE\")\n",
    "    #pipe = make_pipeline(flt,mdl)\n",
    "    #pipe = make_pipeline(FunctionTransformer(FilterMyData), FunctionTransformer(Every_Nth_Value), model)\n",
    "    pipe = Pipeline(steps=[['filter',flt],['nth',nth],['algo',model]])\n",
    "\n",
    "    #print(pipe.get_params().keys())\n",
    "    \n",
    "    # Do gridsearch for svc params\n",
    "    print(\"> GM: GRIDSEARCH\")\n",
    "    #grid = GridSearchCV(pipe, param_grid)\n",
    "    grid = pipe\n",
    "    \n",
    "    return grid\n",
    "\n",
    "def STUFFFROMABOVE(grid, Xtrain, Xtest, ytrain, ytest):\n",
    "    \n",
    "    # Fit model\n",
    "    print(\"> GM: FIT\")\n",
    "    grid.fit(Xtrain, ytrain)\n",
    "    \n",
    "    # Use svc params and predict\n",
    "    print(\"> GM: MAKESTATS\")\n",
    "    moreStats = grid.cv_results_\n",
    "    #print(\"> > Best parameter (CV score=%0.3f):\" % grid.best_score_)\n",
    "    #print(\"> > {}\".format(grid.best_params_))\n",
    "    \n",
    "    # Use svc params and predict\n",
    "    print(\"> GM: PREDICT\")\n",
    "    model = grid.best_estimator_\n",
    "    yfit = model.predict(Xtest)\n",
    "    \n",
    "    # Now that model has done, time for confusion matrix shenanigans\n",
    "    print(\"> GM: CONFUSION\")\n",
    "    mat = confusion_matrix(ytest, yfit)\n",
    "    \n",
    "    return (mat, moreStats)\n",
    "\n",
    "################################\n",
    "\n",
    "def WriteJSON(targetname, acc, pre, rec, stats):\n",
    "    # Preparing the stats text\n",
    "    data = {}\n",
    "    data[targetname] = []\n",
    "    data[targetname].append({\n",
    "        'Accuracy' : acc,\n",
    "        'Precision' : pre,\n",
    "        'Recall' : rec,\n",
    "        'CV Stats': stats\n",
    "    })\n",
    "\n",
    "    # File saving stuff\n",
    "    fname = targetname+\".json\"\n",
    "    targetdest = \"./sktime_results/\"\n",
    "\n",
    "    print(\"Saving {}\".format(fname))\n",
    "\n",
    "    # Write all the info to a file\n",
    "    with open(targetdest+fname, \"w\") as f:\n",
    "        #f.write(stats)\n",
    "        json.dump(data, f, indent=4, default=str)\n",
    "\n",
    "################################\n",
    "\n",
    "def Merge(dict1, dict2):\n",
    "    res = {**dict1, **dict2}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43e24553-fc94-44a2-be8c-f834c346f12a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arsenal',\n",
       " 'CanonicalIntervalForest',\n",
       " 'ContractableBOSS',\n",
       " 'DrCIF',\n",
       " 'IndividualBOSS',\n",
       " 'IndividualTDE',\n",
       " 'MUSE',\n",
       " 'MatrixProfileClassifier',\n",
       " 'RandomIntervalSpectralForest',\n",
       " 'ShapeDTW',\n",
       " 'SignatureClassifier',\n",
       " 'SupervisedTimeSeriesForest',\n",
       " 'TSFreshClassifier',\n",
       " 'WEASEL']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list_of_classifiers = [x.split('./sktime_results/sktime_')[1].split('_fitted.json')[0] for x in glob.glob('./sktime_results/*.json')]\n",
    "list_of_classifiers = [x.split('_')[2] for x in glob.glob('./sktime_results/*.json')]\n",
    "list_of_classifiers.sort()\n",
    "list_of_classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3402a2d3-47ad-4413-9947-86bd49241ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Arsenal': [{'Name': 'Arsenal'}]}\n",
      "{'CanonicalIntervalForest': [{'Name': 'CanonicalIntervalForest'}]}\n",
      "{'ContractableBOSS': [{'Name': 'ContractableBOSS'}]}\n",
      "{'DrCIF': [{'Name': 'DrCIF'}]}\n",
      "{'IndividualBOSS': [{'Name': 'IndividualBOSS'}]}\n",
      "{'IndividualTDE': [{'Name': 'IndividualTDE'}]}\n",
      "{'MUSE': [{'Name': 'MUSE'}]}\n",
      "{'MatrixProfileClassifier': [{'Name': 'MatrixProfileClassifier'}]}\n",
      "{'RandomIntervalSpectralForest': [{'Name': 'RandomIntervalSpectralForest'}]}\n",
      "{'ShapeDTW': [{'Name': 'ShapeDTW'}]}\n",
      "{'SignatureClassifier': [{'Name': 'SignatureClassifier'}]}\n",
      "{'SupervisedTimeSeriesForest': [{'Name': 'SupervisedTimeSeriesForest'}]}\n",
      "{'TSFreshClassifier': [{'Name': 'TSFreshClassifier'}]}\n",
      "{'WEASEL': [{'Name': 'WEASEL'}]}\n"
     ]
    }
   ],
   "source": [
    "def MakeClassifierKWArgs(classifier):\n",
    "    c = {}\n",
    "    c[classifier] = []\n",
    "    c[classifier].append({\n",
    "        \"Name\" : classifier\n",
    "    })\n",
    "    print(c)\n",
    "    return c\n",
    "\n",
    "D = {}\n",
    "for C in list_of_classifiers:\n",
    "    D = Merge(D, MakeClassifierKWArgs(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9ac64a2-cf69-4b49-9e6a-b143c21c2c64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = dict(filter__kw_args = \n",
    "                  [\n",
    "                      {'cutoff': np.linspace(0.00001,0.0018755128487341842)},\n",
    "                      {'order': [1,2,3]},\n",
    "                  ],\n",
    "                  nth__kw_args = \n",
    "                  [\n",
    "                      {'nth': [10, 20, 30, 40, 50]}\n",
    "                  ],\n",
    "                  #flt__cutoff: np.linspace(0.00001,0.0018755128487341842),\n",
    "                  #flt__order: [1,2,3]   #,\n",
    "                  #drcif__base_estimator = ['DTC', 'CIT'],\n",
    "                  #drcif__n_estimators   =   np.linspace(100,1000,19)\n",
    "             )\n",
    "\n",
    "param_grid = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa90d1fd-d359-4f0a-854e-7f951522c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnx = list(np.array(from_nested_to_2d_array(Xtrain)))\n",
    "#fnx = fnx.tolist()\n",
    "fnx[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc1abc-5e2e-4a19-b6d4-a0559b68ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02b01a1-c622-49d6-9373-cfc12ca039b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> GM: START\n",
      "\t> Model: Arsenal\n",
      "> GM: GENERATING TRANSFORMERS\n",
      "> GM: MAKE PIPELINE\n",
      "> GM: GRIDSEARCH\n"
     ]
    }
   ],
   "source": [
    "#GetMetrics(Arsenal, from_nested_to_2d_array(Xtrain), Xtest, ytrain, ytest, param_grid)\n",
    "#G = GetMetrics(Arsenal, Xtrain_, Xtest_, ytrain_, ytest_, param_grid)\n",
    "G = GetMetrics(Arsenal, Xtrain_, Xtest_, ytrain_, ytest_, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec488340-8b72-47c6-9744-63076b36d997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> GM: FIT\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [172, 6865]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_196576/1348295312.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSTUFFFROMABOVE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtrain_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtest_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytest_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_196576/3381032255.py\u001b[0m in \u001b[0;36mSTUFFFROMABOVE\u001b[0;34m(grid, Xtrain, Xtest, ytrain, ytest)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;31m# Fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"> GM: FIT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;31m# Use svc params and predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sktime/classification/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mcoerce_to_pandas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"coerce-X-to-pandas\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mallow_multivariate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"capability:multivariate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         X, y = check_X_y(\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sktime/utils/validation/panel.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, enforce_univariate, enforce_min_instances, enforce_min_columns, coerce_to_numpy, coerce_to_pandas)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# only check y for the minimum number of instances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce_to_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_to_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     X = check_X(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [172, 6865]"
     ]
    }
   ],
   "source": [
    "Gg = STUFFFROMABOVE(G, Xtrain_, Xtest_, ytrain_, ytest_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f76b37a-a9ce-4f44-9a3d-320cf51e6c93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4236433f-f709-447e-9aa3-13f760f71713",
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier in list_of_classifiers:\n",
    "        print(f\"Model: {classifier}\")\n",
    "        c = eval(classifier)()\n",
    "        print(type(c).__name__)\n",
    "        #Parallel(n_jobs=10)(delayed(MakeModels)(X_train,y_train,c) for c in classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237f926f-ce9f-4365-9fee-1a257463c503",
   "metadata": {},
   "source": [
    "### Refreshing memory of SKLearn Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a331d-15d3-434d-826c-c63065fab5a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16,5)\n",
    "\n",
    "Z = rawX[0]\n",
    "plt.plot(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f938a30-6ba5-4f4b-8f3a-a0c56b1cce62",
   "metadata": {},
   "source": [
    "#### Seeing if any functions from preprocessing do what I want to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56310368-ce9e-42df-8b78-cbaabb3a316b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Normalise(X,fixnan=True):\n",
    "    \n",
    "    # First of all, decide if wan to Fix all the 0s / NaNs\n",
    "    if fixnan:\n",
    "        X = FIXNAN(X)\n",
    "    \n",
    "    # Reshape because apparently it doesn't work if I don't\n",
    "    n = np.reshape(X, (-1,1))\n",
    "    \n",
    "    # Make and fit scaler\n",
    "    scaler = StandardScaler() #(0.75,1.25))\n",
    "    scaler.fit(n)\n",
    "    \n",
    "    # Calculate values\n",
    "    #scMean = scaler.mean_\n",
    "    scTrans = scaler.transform(n)    # <--- +1 to center it on y=1 rather than y=0\n",
    "    \n",
    "    # Return transformed array\n",
    "    return scTrans\n",
    "\n",
    "def MaxABS(X,fixnan=True,center=1):\n",
    "    # First of all, decide if wan to Fix all the 0s / NaNs\n",
    "    if fixnan:\n",
    "        X = FIXNAN(X)\n",
    "    \n",
    "    n = np.reshape(X, (-1,1))\n",
    "    scaler = MaxAbsScaler() #(0.75,1.25))\n",
    "    scaler.fit(n)\n",
    "    scTrans = scaler.transform(n)\n",
    "    #Median = np.median(scTrans)\n",
    "    #scTrans = scTrans + (center-Median)     # <-- Center on one\n",
    "    return scTrans\n",
    "\n",
    "def Normal(X,fixnan=True):\n",
    "    # First of all, decide if wan to Fix all the 0s / NaNs\n",
    "    if fixnan:\n",
    "        X = FIXNAN(X)\n",
    "    \n",
    "    median = np.median(X)\n",
    "    \n",
    "    #print(f\"OldNormal median = {median}\")\n",
    "    \n",
    "    X[:] = [(number/median) for number in X]\n",
    "    return X\n",
    "\n",
    "NormaliseData = FunctionTransformer(Normal)\n",
    "\n",
    "A = rawX[0]\n",
    "B = rawX[1]\n",
    "C = rawX[2]\n",
    "zA = MaxABS(A)\n",
    "zB = MaxABS(B)\n",
    "zC = MaxABS(C)\n",
    "nA = NormaliseData.transform(A)\n",
    "nB = NormaliseData.transform(B)\n",
    "nC = NormaliseData.transform(C)\n",
    "\n",
    "print(f\"Medians:\\tMedian(zA) =\\t{np.median(zA)}\\n\\t\\tMedian(nA) =\\t{np.median(nA)}\\n\")\n",
    "print(f\"Medians:\\tMedian(zB) =\\t{np.median(zB)}\\n\\t\\tMedian(nB) =\\t{np.median(nB)}\\n\")\n",
    "print(f\"Medians:\\tMedian(zC) =\\t{np.median(zC)}\\n\\t\\tMedian(nC) =\\t{np.median(nC)}\\n\")\n",
    "\n",
    "#plt.plot(zA)\n",
    "plt.plot(zA, 'b-')\n",
    "#plt.plot(zC)\n",
    "plt.plot(nA, 'g-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04333978-ed93-46ae-b726-63f3d1d37ab6",
   "metadata": {},
   "source": [
    "#### So it seems that nothing really does what I want it to do; MaxAbsScaler comes close but isn't quite on the ball. Seeing as it's close enough, this might work, but for now I'm sticking with making my own transformer out of the normalise function, so I can use it in Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b42727-7630-4c98-9cc4-0bcdf2f216a3",
   "metadata": {},
   "source": [
    "### ON TO MAIN STUFF!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbac99d5-df56-4063-aca3-49208f277b64",
   "metadata": {},
   "source": [
    "Order of steps:\n",
    "* Load X and Y\n",
    "* Train-test-split them (use nested for SKTime? Will work with Nested Array? Checked, both t-t-s sets contain same data)\n",
    "* Start Loop:\n",
    "\n",
    "- For each classifier:\n",
    "  1. \n",
    "  2. B\n",
    "  3. C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab4ba0d-c641-441f-80c0-3cbf075c72c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(np.linspace(0.00001,0.0018755128487341842))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bde0c88-5d74-4e77-bcc5-93c2a937462e",
   "metadata": {},
   "source": [
    "So, taking \"raw\" data (ie, not normalised) and applying a `scaler.mean__` function to it does the same thing as my normalising function.  \n",
    "Therefore, I should use the raw data (ie, nothing done to it) and then apply the scaler as part of the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06c38e4-73ce-485d-8f2d-265fd8a5075a",
   "metadata": {},
   "source": [
    "## GRIDSEARCHING SKTIME\n",
    "Please refer to https://www.sktime.org/en/stable/api_reference/auto_generated/sktime.forecasting.model_selection.ForecastingGridSearchCV.html  \n",
    "and  \n",
    "https://www.sktime.org/en/stable/examples/01_forecasting.html#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8019592b-4f40-40f9-99d9-2b5780eb06b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "na,nb,nc,nd = train_test_split(X_nested, masterY, random_state=42)\n",
    "ma,mb,mc,md = train_test_split( masterX, masterY, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766f01de-760b-4077-9c9c-293f74792c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c05497-8f18-4ec7-8f3c-05daaa4b2f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c678091-52be-48cc-ae38-50eaab083fdf",
   "metadata": {},
   "source": [
    "### They are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a3751e-7671-4e50-8c14-7017f22635c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
